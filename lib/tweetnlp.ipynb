{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "Welcome everyone! This notebook is intended to demo the tweetnlp \n",
    "module. \n",
    "\n",
    "# What is TweetNLP?\n",
    "I wrote tweetnlp because I wanted to try out keras and tensorflow, \n",
    "and I'm interested in natural language processing. I enjoy reading \n",
    "twitter, and I've always enjoyed how each person on twitter can have\n",
    "such a distinctive voice, even with so few words.  So I wondered \n",
    "if you could use tensortflow to build models for writing tweets in\n",
    "the style of an existing twitter user. \n",
    "\n",
    "The tweetnlp model does this (sort of...). It has an API for\n",
    "downloading tweets using a twitter app credentials (you need \n",
    "to apply to be a twitter developer to make an app). It then \n",
    "will process the tweets into text, and build a machine learning \n",
    "model based on those tweets. You can then either generate tweets \n",
    "individually, or predict tweets word-by-word, potentially inserting \n",
    "your own topics that way. \n",
    "\n",
    "# Demo\n",
    "This code is pretty easy to use. The downloading and model \n",
    "building functions take several configuration options which \n",
    "may or may not make your predictions better.  \n",
    "\n",
    "Let's make a model with my favorite twitter account, @dog_rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweetnlp\n",
    "import shutil\n",
    "\n",
    "data_dir = './temp'\n",
    "# get rid of old data\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "\n",
    "os.mkdir(data_dir)\n",
    "\n",
    "raw_file = os.path.join(data_dir, 'raw')\n",
    "# download the tweets to ./temp/raw\n",
    "tweetnlp.download_tweets('dog_rates', raw_file, exclude_replies=True)\n",
    "\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We've now downloaded the tweets to a file. Each line of this file\n",
    "is raw json, here's an example:\n",
    "```json\n",
    "{\"created_at\": \"Mon Sep 16 16:21:43 +0000 2019\", \"favorite_count\": 111102, \"full_text\": \"This is Apollo. He likes to point at those with beautiful smiles who are going to have a great day today. 13/10 https://t.co/oCdx6Rn3PQ\", \"hashtags\": [], \"id\": 1173633070630498309, \"id_str\": \"1173633070630498309\", \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/oCdx6Rn3PQ\", \"expanded_url\": \"https://twitter.com/dog_rates/status/1173633070630498309/photo/1\", \"id\": 1173633065635074048, \"media_url\": \"http://pbs.twimg.com/media/EEmVDhXU0AA5XOV.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/EEmVDhXU0AA5XOV.jpg\", \"sizes\": {\"thumb\": {\"w\": 150, \"h\": 150, \"resize\": \"crop\"}, \"medium\": {\"w\": 682, \"h\": 776, \"resize\": \"fit\"}, \"small\": {\"w\": 598, \"h\": 680, \"resize\": \"fit\"}, \"large\": {\"w\": 682, \"h\": 776, \"resize\": \"fit\"}}, \"type\": \"photo\", \"url\": \"https://t.co/oCdx6Rn3PQ\"}, {\"display_url\": \"pic.twitter.com/oCdx6Rn3PQ\", \"expanded_url\": \"https://twitter.com/dog_rates/status/1173633070630498309/photo/1\", \"id\": 1173633065643470848, \"media_url\": \"http://pbs.twimg.com/media/EEmVDhZU8AAWHxp.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/EEmVDhZU8AAWHxp.jpg\", \"sizes\": {\"thumb\": {\"w\": 150, \"h\": 150, \"resize\": \"crop\"}, \"medium\": {\"w\": 759, \"h\": 924, \"resize\": \"fit\"}, \"large\": {\"w\": 759, \"h\": 924, \"resize\": \"fit\"}, \"small\": {\"w\": 559, \"h\": 680, \"resize\": \"fit\"}}, \"type\": \"photo\", \"url\": \"https://t.co/oCdx6Rn3PQ\"}], \"retweet_count\": 14172, \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \"urls\": [], \"user\": {\"id\": 4196983835, \"id_str\": \"4196983835\"}, \"user_mentions\": []}\n",
    "```\n",
    "From there, we generate a separate file that's just the text, with\n",
    "urls and mentions removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweets', 'raw']\n"
     ]
    }
   ],
   "source": [
    "tweets_file = os.path.join(data_dir, 'tweets')\n",
    "tweetnlp.generate_tweets_text(raw_file, tweets_file)\n",
    "\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This now just has the sanitized tweet text like so:\n",
    "```\n",
    "This is Apollo. He likes to point at those with beautiful smiles who are going to have a great day today. 13/10\n",
    "```\n",
    "Finally, we build the model itself.  Here we're using the defaults except\n",
    "for the embedding_dim, which I've found was good when it was at 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3112\n",
      "Total Sequences: 15341\n",
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 50)             155600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3112)              158712    \n",
      "=================================================================\n",
      "Total params: 334,512\n",
      "Trainable params: 334,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/colindr/.virtualenvs/nlpservice/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/50\n",
      " - 4s - loss: 6.5863 - acc: 0.0694\n",
      "Epoch 2/50\n",
      " - 3s - loss: 5.8446 - acc: 0.0894\n",
      "Epoch 3/50\n",
      " - 3s - loss: 5.6147 - acc: 0.1206\n",
      "Epoch 4/50\n",
      " - 4s - loss: 5.4494 - acc: 0.1564\n",
      "Epoch 5/50\n",
      " - 4s - loss: 5.2881 - acc: 0.1728\n",
      "Epoch 6/50\n",
      " - 4s - loss: 5.0972 - acc: 0.1866\n",
      "Epoch 7/50\n",
      " - 4s - loss: 4.8951 - acc: 0.2023\n",
      "Epoch 8/50\n",
      " - 4s - loss: 4.6966 - acc: 0.2211\n",
      "Epoch 9/50\n",
      " - 4s - loss: 4.5081 - acc: 0.2346\n",
      "Epoch 10/50\n",
      " - 4s - loss: 4.3400 - acc: 0.2510\n",
      "Epoch 11/50\n",
      " - 4s - loss: 4.1869 - acc: 0.2618\n",
      "Epoch 12/50\n",
      " - 4s - loss: 4.0473 - acc: 0.2710\n",
      "Epoch 13/50\n",
      " - 4s - loss: 3.9160 - acc: 0.2838\n",
      "Epoch 14/50\n",
      " - 4s - loss: 3.7921 - acc: 0.2976\n",
      "Epoch 15/50\n",
      " - 4s - loss: 3.6736 - acc: 0.3099\n",
      "Epoch 16/50\n",
      " - 4s - loss: 3.5625 - acc: 0.3277\n",
      "Epoch 17/50\n",
      " - 4s - loss: 3.4521 - acc: 0.3434\n",
      "Epoch 18/50\n",
      " - 4s - loss: 3.3506 - acc: 0.3585\n",
      "Epoch 19/50\n",
      " - 4s - loss: 3.2490 - acc: 0.3733\n",
      "Epoch 20/50\n",
      " - 4s - loss: 3.1513 - acc: 0.3916\n",
      "Epoch 21/50\n",
      " - 4s - loss: 3.0564 - acc: 0.4063\n",
      "Epoch 22/50\n",
      " - 4s - loss: 2.9645 - acc: 0.4189\n",
      "Epoch 23/50\n",
      " - 4s - loss: 2.8755 - acc: 0.4377\n",
      "Epoch 24/50\n",
      " - 4s - loss: 2.7898 - acc: 0.4496\n",
      "Epoch 25/50\n",
      " - 4s - loss: 2.7073 - acc: 0.4667\n",
      "Epoch 26/50\n",
      " - 4s - loss: 2.6272 - acc: 0.4796\n",
      "Epoch 27/50\n",
      " - 4s - loss: 2.5493 - acc: 0.4941\n",
      "Epoch 28/50\n",
      " - 4s - loss: 2.4736 - acc: 0.5071\n",
      "Epoch 29/50\n",
      " - 4s - loss: 2.4002 - acc: 0.5232\n",
      "Epoch 30/50\n",
      " - 4s - loss: 2.3318 - acc: 0.5320\n",
      "Epoch 31/50\n",
      " - 4s - loss: 2.2645 - acc: 0.5447\n",
      "Epoch 32/50\n",
      " - 4s - loss: 2.2008 - acc: 0.5549\n",
      "Epoch 33/50\n",
      " - 4s - loss: 2.1396 - acc: 0.5653\n",
      "Epoch 34/50\n",
      " - 4s - loss: 2.0816 - acc: 0.5765\n",
      "Epoch 35/50\n",
      " - 4s - loss: 2.0252 - acc: 0.5841\n",
      "Epoch 36/50\n",
      " - 4s - loss: 1.9722 - acc: 0.5925\n",
      "Epoch 37/50\n",
      " - 4s - loss: 1.9218 - acc: 0.6002\n",
      "Epoch 38/50\n",
      " - 4s - loss: 1.8737 - acc: 0.6097\n",
      "Epoch 39/50\n",
      " - 3s - loss: 1.8276 - acc: 0.6150\n",
      "Epoch 40/50\n",
      " - 4s - loss: 1.7832 - acc: 0.6217\n",
      "Epoch 41/50\n",
      " - 4s - loss: 1.7440 - acc: 0.6281\n",
      "Epoch 42/50\n",
      " - 4s - loss: 1.7030 - acc: 0.6337\n",
      "Epoch 43/50\n",
      " - 4s - loss: 1.6654 - acc: 0.6410\n",
      "Epoch 44/50\n",
      " - 4s - loss: 1.6295 - acc: 0.6474\n",
      "Epoch 45/50\n",
      " - 3s - loss: 1.5950 - acc: 0.6481\n",
      "Epoch 46/50\n",
      " - 4s - loss: 1.5626 - acc: 0.6560\n",
      "Epoch 47/50\n",
      " - 4s - loss: 1.5309 - acc: 0.6595\n",
      "Epoch 48/50\n",
      " - 3s - loss: 1.5020 - acc: 0.6651\n",
      "Epoch 49/50\n",
      " - 3s - loss: 1.4739 - acc: 0.6664\n",
      "Epoch 50/50\n",
      " - 3s - loss: 1.4459 - acc: 0.6715\n",
      "['tokenizer', 'tweets', 'model', 'raw']\n"
     ]
    }
   ],
   "source": [
    "import tweetnlp.model\n",
    "model_file = os.path.join(data_dir, 'model')\n",
    "tokenizer_file = os.path.join(data_dir, 'tokenizer')\n",
    "# this function will write to both the model file and the tokenizer file\n",
    "# this process can take a long time. \n",
    "tweetnlp.model.build_tweet_model(tweets_file, model_file, tokenizer_file, embedding_dim=50)\n",
    "\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was ok!  Now lets generate a tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is archie it’s his first time today had you to know he’s a smiley boy hopes you like it 12 10\n"
     ]
    }
   ],
   "source": [
    "print(tweetnlp.model.tweet_from_model(model_file, tokenizer_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do word-by-word predictions like so:\n",
    "```\n",
    "python -m tweetnlp predict temp/model temp/tokenizer\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
